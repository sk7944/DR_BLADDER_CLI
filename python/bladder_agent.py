#!/usr/bin/env python3
"""
BladderCancerAgent - ë°©ê´‘ì•” EAU ê°€ì´ë“œë¼ì¸ AI Agent í•µì‹¬ í´ë˜ìŠ¤
Ollama Qwen ëª¨ë¸ê³¼ RAG ê¸°ëŠ¥ì„ í†µí•©í•œ ë…ë¦½ ì—ì´ì „íŠ¸
"""

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import unicodedata
import re
import time

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import ollama
import torch
import chromadb
import PyPDF2
from sentence_transformers import SentenceTransformer
from chromadb.utils import embedding_functions
from tqdm import tqdm
import psutil

class BladderCancerAgent:
    """ë°©ê´‘ì•” EAU ê°€ì´ë“œë¼ì¸ AI Agent í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.is_initialized = False
        self.ollama_client = None
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        self.pdf_loaded = False
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config.cache_dir, exist_ok=True)
        
        # ì¸ì½”ë”© ì„¤ì •
        self._setup_encoding()
        
        # í•œêµ­ì–´-ì˜ì–´ ì˜ë£Œìš©ì–´ ë§¤í•‘
        self.korean_to_english = {
            "ë°©ê´‘ì•”": "bladder cancer urothelial carcinoma",
            "ë¹„ê·¼ì¹¨ìœ¤ì„±": "non-muscle invasive NMIBC",
            "ê·¼ì¹¨ìœ¤ì„±": "muscle invasive MIBC",
            "BCG": "BCG bacillus calmette guerin",
            "ë°©ê´‘ê²½": "cystoscopy",
            "ê²½ìš”ë„ë°©ê´‘ì¢…ì–‘ì ˆì œìˆ ": "TURBT transurethral resection",
            "í™”í•™ìš”ë²•": "chemotherapy",
            "ë©´ì—­ìš”ë²•": "immunotherapy",
            "ì¬ë°œ": "recurrence",
            "ë¶€ì‘ìš©": "side effects adverse",
            "ì¹˜ë£Œ": "treatment therapy",
            "ìˆ˜ìˆ ": "surgery operation",
            "ì§„ë‹¨": "diagnosis",
            "ì˜ˆí›„": "prognosis",
            "ìƒì¡´ìœ¨": "survival rate",
            "ë³‘ê¸°": "stage staging",
            "ë“±ê¸‰": "grade grading",
            "ìœ„í—˜ë„": "risk factors",
            "ê°€ì´ë“œë¼ì¸": "guidelines recommendations",
            "í‘œì¤€ì¹˜ë£Œ": "standard treatment",
            "í•­ì•”": "anticancer chemotherapy",
            "ë°©ì‚¬ì„ ": "radiation radiotherapy",
            "ì ˆì œ": "resection removal",
            "ìƒê²€": "biopsy",
            "ì¡°ì§ê²€ì‚¬": "histopathology",
            "ì¢…ì–‘": "tumor neoplasm",
            "ì•…ì„±": "malignant",
            "ì–‘ì„±": "benign",
            "ì „ì´": "metastasis",
            "ì¹¨ìœ¤": "invasion",
            "ìƒí”¼": "epithelium urothelial",
            "ìš”ë¡œ": "urinary tract",
            "ë°°ë‡¨": "urination voiding",
            "í˜ˆë‡¨": "hematuria",
            "ë¹ˆë‡¨": "frequency",
            "ìš”ì ˆë°•": "urgency",
            "ìš”ì‹¤ê¸ˆ": "incontinence"
        }
        
        # ë°©ê´‘ì•” ê´€ë ¨ í‚¤ì›Œë“œ (ê¸°ì¡´ MCP ì„œë²„ì—ì„œ ê°€ì ¸ì˜´)
        self.bladder_keywords = [
            "BCG", "TURBT", "NMIBC", "MIBC", "bladder", "cancer", "urothelial", "carcinoma",
            "cystoscopy", "resection", "chemotherapy", "immunotherapy", "recurrence",
            "survival", "prognosis", "stage", "grade", "risk", "treatment", "therapy",
            "guidelines", "recommendation", "EAU", "urinary", "tract", "hematuria",
            "frequency", "urgency", "incontinence", "biopsy", "histopathology",
            "invasion", "metastasis", "epithelium", "malignant", "benign"
        ]

    def _setup_encoding(self):
        """ì¸ì½”ë”© ì„¤ì •"""
        try:
            import locale
            import sys
            
            # ë¡œì¼€ì¼ ì„¤ì •
            if os.name == 'nt':  # Windows
                try:
                    locale.setlocale(locale.LC_ALL, 'Korean_Korea.utf8')
                except:
                    try:
                        locale.setlocale(locale.LC_ALL, 'ko_KR.utf8')
                    except:
                        locale.setlocale(locale.LC_ALL, 'en_US.utf8')
            else:  # Linux/macOS
                try:
                    locale.setlocale(locale.LC_ALL, 'ko_KR.utf8')
                except:
                    try:
                        locale.setlocale(locale.LC_ALL, 'en_US.utf8')
                    except:
                        locale.setlocale(locale.LC_ALL, 'C.UTF-8')
        except Exception as e:
            self.logger.warning(f"ë¡œì¼€ì¼ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        os.environ['LANG'] = 'en_US.UTF-8'
        os.environ['LC_ALL'] = 'en_US.UTF-8'
        
        # Python ìŠ¤íŠ¸ë¦¼ ì¸ì½”ë”© ì„¤ì •
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8')
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8')
        if hasattr(sys.stdin, 'reconfigure'):
            sys.stdin.reconfigure(encoding='utf-8')

    def initialize(self) -> bool:
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘")
            
            # 1. Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if not self._init_ollama():
                return False
            
            # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            if not self._init_embedding_model():
                return False
            
            # 3. ChromaDB ì´ˆê¸°í™”
            if not self._init_chromadb():
                return False
            
            # 4. PDF ë¡œë“œ ë° ë²¡í„°í™”
            if not self._load_pdf_and_vectorize():
                return False
            
            self.is_initialized = True
            self.logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _init_ollama(self) -> bool:
        """Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            
            # Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.ollama_client = ollama.Client(host=self.config.ollama_host)
            
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸
            models = self.ollama_client.list()
            self.logger.info(f"Ollama ì‘ë‹µ: {models}")
            
            # ì•ˆì „í•œ ëª¨ë¸ ëª©ë¡ ì¶”ì¶œ
            if 'models' in models and isinstance(models['models'], list):
                model_names = []
                for model in models['models']:
                    if isinstance(model, dict) and 'name' in model:
                        model_names.append(model['name'])
                    else:
                        self.logger.warning(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ ëª¨ë¸ í˜•ì‹: {model}")
            else:
                self.logger.warning(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ Ollama ì‘ë‹µ í˜•ì‹: {models}")
                model_names = []
            
            if self.config.model_name not in model_names:
                self.logger.info(f"ëª¨ë¸ '{self.config.model_name}'ì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                self.logger.info(f"ì„¤ì¹˜ëœ ëª¨ë¸: {model_names}")
                
                # ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
                if not self._download_model():
                    return False
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            response = self.ollama_client.generate(
                model=self.config.model_name,
                prompt="Hello",
                stream=False
            )
            
            if not response.get('response'):
                self.logger.error("Ollama ëª¨ë¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"Ollama ëª¨ë¸ '{self.config.model_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _download_model(self) -> bool:
        """Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"ëª¨ë¸ '{self.config.model_name}' ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 1GB)")
            print("ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("-" * 60)
            
            # Ollama pull ëª…ë ¹ ì‹¤í–‰
            import subprocess
            import sys
            import re
            
            # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•œ Popen ì‚¬ìš©
            process = subprocess.Popen(
                ['ollama', 'pull', self.config.model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True,
                encoding='utf-8',
                errors='replace'
            )
            
            # ì§„í–‰ìƒí™© ì¶”ì  ë³€ìˆ˜
            current_step = "ì¤€ë¹„ ì¤‘"
            progress_count = 0
            last_significant_line = ""
            
            # ë‹¨ìˆœí•œ ì§„í–‰ í‘œì‹œê¸°
            with tqdm(desc=current_step, bar_format='{desc}: {n}/100 {bar}', total=100) as pbar:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # ì¤‘ìš”í•œ ë¼ì¸ë§Œ ì¶œë ¥
                    is_important = False
                    
                    # pulling manifest, config, model ë“± ì£¼ìš” ë‹¨ê³„ ê°ì§€
                    if 'pulling manifest' in line.lower():
                        current_step = "Downloading manifest"
                        pbar.set_description(current_step)
                        pbar.update(10)
                        is_important = True
                    elif 'pulling' in line.lower() and 'sha256:' in line.lower():
                        if 'config' in line.lower():
                            current_step = "Downloading config"
                        else:
                            current_step = "Downloading model layers"
                        pbar.set_description(current_step)
                        progress_count += 15
                        pbar.update(15)
                        is_important = True
                    elif 'verifying' in line.lower():
                        current_step = "Verifying download"
                        pbar.set_description(current_step)
                        pbar.update(10)
                        is_important = True
                    elif 'success' in line.lower() or 'complete' in line.lower():
                        current_step = "Download complete"
                        pbar.set_description(current_step)
                        pbar.update(100 - pbar.n)  # ë‚˜ë¨¸ì§€ ëª¨ë‘ ì±„ìš°ê¸°
                        is_important = True
                    
                    # ì¤‘ìš”í•œ ë¼ì¸ë§Œ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                    if is_important and line != last_significant_line:
                        print(f"  {line}")
                        last_significant_line = line
            
            process.wait()
            print("-" * 60)
            
            if process.returncode == 0:
                print(f"ëª¨ë¸ '{self.config.model_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {process.returncode})")
                return False
                
        except subprocess.TimeoutExpired:
            print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def _init_embedding_model(self) -> bool:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            model_name = self.config.embedding_model
            self.embedding_model = SentenceTransformer(model_name, device=device)
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_text = "BCG ì¹˜ë£Œì˜ ë¶€ì‘ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            embedding = self.embedding_model.encode([test_text])
            
            if embedding is None or len(embedding) == 0:
                self.logger.error("ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _init_chromadb(self) -> bool:
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            self.logger.info("ChromaDB ì´ˆê¸°í™” ì¤‘...")
            
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.chroma_client = chromadb.PersistentClient(
                path=str(Path(self.config.cache_dir) / "chroma_db")
            )
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
            collection_name = "bladder_cancer_guidelines"
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ (ì¬ìƒì„±)
            try:
                self.chroma_client.delete_collection(collection_name)
            except:
                pass
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ì„ë² ë”© í•¨ìˆ˜ ì—†ì´ ìƒì„±)
            self.collection = self.chroma_client.create_collection(
                name=collection_name
            )
            
            self.logger.info("ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _load_pdf_and_vectorize(self) -> bool:
        """PDF ë¡œë“œ ë° ë²¡í„°í™”"""
        try:
            self.logger.info("PDF ë¡œë“œ ë° ë²¡í„°í™” ì‹œì‘...")
            print("ğŸ“„ PDF ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # PDF íŒŒì¼ ê²½ë¡œ
            pdf_path = Path(self.config.pdf_path)
            if not pdf_path.exists():
                self.logger.error(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                return False
            
            print(f"ğŸ“– PDF íŒŒì¼ í¬ê¸°: {pdf_path.stat().st_size / (1024*1024):.1f}MB")
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            print("ğŸ” PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            documents = self._extract_pdf_text(pdf_path)
            if not documents:
                self.logger.error("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                print("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
            
            print(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(documents)}ê°œ í˜ì´ì§€")
            
            # ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§
            print("ğŸ” ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§ ì¤‘...")
            filtered_docs = self._filter_relevant_documents(documents)
            if not filtered_docs:
                self.logger.error("ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                print("âŒ ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"âœ… ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§ ì™„ë£Œ: {len(filtered_docs)}ê°œ ë¬¸ì„œ")
            
            # ë²¡í„°í™” ë° ChromaDBì— ì €ì¥
            print("ğŸ§  ë¬¸ì„œ ë²¡í„°í™” ë° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
            self._vectorize_and_store(filtered_docs)
            
            self.pdf_loaded = True
            self.logger.info(f"PDF ë²¡í„°í™” ì™„ë£Œ: {len(filtered_docs)}ê°œ ë¬¸ì„œ")
            print("âœ… PDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
            
        except Exception as e:
            self.logger.error(f"PDF ë¡œë“œ ë° ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return False

    def _extract_pdf_text(self, pdf_path: Path) -> List[str]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            documents = []
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                total_pages = len(pdf_reader.pages)
                print(f"ğŸ“„ ì´ {total_pages}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘...")
                
                for page_num in tqdm(range(total_pages), desc="í˜ì´ì§€ ì²˜ë¦¬"):
                    try:
                        page = pdf_reader.pages[page_num]
                        text = page.extract_text()
                        
                        if text and text.strip():
                            # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                            try:
                                # UTF-8 ì¸ì½”ë”© í™•ì¸ ë° ë³€í™˜
                                if isinstance(text, bytes):
                                    text = text.decode('utf-8', errors='ignore')
                                elif isinstance(text, str):
                                    # ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                                    text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                                
                                # í…ìŠ¤íŠ¸ ì •ë¦¬
                                cleaned_text = self._clean_text(text)
                                if cleaned_text and len(cleaned_text.strip()) > 10:
                                    documents.append(cleaned_text)
                                    
                            except UnicodeError as e:
                                self.logger.warning(f"í˜ì´ì§€ {page_num} ì¸ì½”ë”© ì˜¤ë¥˜, ê±´ë„ˆëœ€: {e}")
                                continue
                                
                    except Exception as e:
                        self.logger.warning(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {e}")
                        continue
                
                print(f"ğŸ“„ í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ì¶”ì¶œ")
            
            return documents
            
        except Exception as e:
            self.logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        try:
            # ì•ˆì „í•œ ì¸ì½”ë”© ì²˜ë¦¬
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='ignore')
            
            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
            text = unicodedata.normalize('NFC', text)
            
            # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° (ë” ì•ˆì „í•œ íŒ¨í„´)
            # ì˜ë¬¸, ìˆ«ì, í•œê¸€, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ìœ ì§€
            text = re.sub(r'[^\w\s\-\.\,\;\:\(\)\[\]\{\}\'\"\/\%\&\+\=\<\>\!\?\uAC00-\uD7A3]', ' ', text, flags=re.UNICODE)
            
            # ì—°ì†ëœ ê³µë°± ì œê±°
            text = re.sub(r'\s+', ' ', text)
            
            # ì•ë’¤ ê³µë°± ì œê±°
            text = text.strip()
            
            return text
            
        except Exception as e:
            self.logger.warning(f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì•ˆì „í•œ ë²„ì „ ë°˜í™˜
            try:
                return str(text).encode('ascii', errors='ignore').decode('ascii')
            except:
                return ""

    def _filter_relevant_documents(self, documents: List[str]) -> List[str]:
        """ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§"""
        filtered = []
        
        for doc in documents:
            # í‚¤ì›Œë“œ ë§¤ì¹­
            doc_lower = doc.lower()
            if any(keyword.lower() in doc_lower for keyword in self.bladder_keywords):
                # ìµœì†Œ ê¸¸ì´ í™•ì¸
                if len(doc.split()) >= 10:
                    filtered.append(doc)
        
        return filtered

    def _vectorize_and_store(self, documents: List[str]):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ChromaDB ì €ì¥"""
        try:
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë” ì‘ì€ ë°°ì¹˜ë¡œ ì‹œì‘)
            batch_size = 8  # ê¸°ë³¸ê°’ì„ ë” ì‘ê²Œ ì„¤ì •
            if os.name == 'nt':  # Windows
                batch_size = min(batch_size, 4)  # Windowsì—ì„œëŠ” 4ë¡œ ì œí•œ
            
            print(f"ë¬¸ì„œ ë²¡í„°í™” ì‹œì‘... (ì´ {len(documents)}ê°œ ë¬¸ì„œ, ë°°ì¹˜ í¬ê¸°: {batch_size})")
            
            for i in tqdm(range(0, len(documents), batch_size), desc="ë¬¸ì„œ ë²¡í„°í™”"):
                try:
                    batch = documents[i:i+batch_size]
                    batch_num = i // batch_size + 1
                    
                    print(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ë¬¸ì„œ)")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    memory_percent = psutil.virtual_memory().percent
                    if memory_percent > 70:
                        print(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤ ({memory_percent:.1f}%). ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.")
                        batch_size = max(1, batch_size // 2)
                        batch = documents[i:i+batch_size]
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # ì„ë² ë”© ìƒì„± (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
                    print(f"ë°°ì¹˜ {batch_num} ì„ë² ë”© ìƒì„± ì¤‘...")
                    start_time = time.time()
                    
                    # ë¬¸ì„œ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ë¬¸ì„œëŠ” ì˜ë¼ë‚´ê¸°)
                    max_length = 1000
                    truncated_batch = []
                    for doc in batch:
                        if len(doc) > max_length:
                            truncated_batch.append(doc[:max_length] + "...")
                        else:
                            truncated_batch.append(doc)
                    
                    embeddings = self.embedding_model.encode(
                        truncated_batch, 
                        convert_to_tensor=False,
                        show_progress_bar=False,
                        batch_size=min(4, len(truncated_batch))  # ë‚´ë¶€ ë°°ì¹˜ í¬ê¸°ë„ ì œí•œ
                    )
                    
                    elapsed_time = time.time() - start_time
                    print(f"ë°°ì¹˜ {batch_num} ì„ë² ë”© ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                    
                    # ChromaDBì— ì €ì¥
                    print(f"ë°°ì¹˜ {batch_num} ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
                    ids = [f"doc_{i+j}" for j in range(len(batch))]
                    
                    # ì„ë² ë”©ì„ numpy arrayë¡œ ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    import numpy as np
                    if hasattr(embeddings, 'numpy'):
                        embeddings_list = embeddings.numpy().tolist()
                    elif isinstance(embeddings, np.ndarray):
                        embeddings_list = embeddings.tolist()
                    else:
                        embeddings_list = embeddings.tolist()
                    
                    # ê° ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì¶”ê°€ (ì¼ê´„ ì¶”ê°€ê°€ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ)
                    for j, (embedding, doc, doc_id) in enumerate(zip(embeddings_list, batch, ids)):
                        try:
                            print(f"  ë¬¸ì„œ {j+1}/{len(batch)} ì €ì¥ ì¤‘...")
                            self.collection.add(
                                embeddings=[embedding],
                                documents=[doc],
                                ids=[doc_id]
                            )
                            print(f"  ë¬¸ì„œ {j+1} ì €ì¥ ì™„ë£Œ")
                        except Exception as doc_error:
                            print(f"  ë¬¸ì„œ {j+1} ì €ì¥ ì‹¤íŒ¨: {str(doc_error)}")
                            self.logger.error(f"ë¬¸ì„œ {doc_id} ì €ì¥ ì‹¤íŒ¨: {str(doc_error)}")
                            continue
                    
                    print(f"ë°°ì¹˜ {batch_num} ì™„ë£Œ")
                    
                    # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì•ˆì •í™”)
                    time.sleep(0.1)
                    
                except MemoryError as e:
                    self.logger.error(f"ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ (ë°°ì¹˜ {batch_num}): {str(e)}")
                    print(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë²¡í„°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    # ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ì„œ ì¬ì‹œë„
                    batch_size = max(1, batch_size // 2)
                    if batch_size == 1:
                        raise
                    continue
                except Exception as e:
                    self.logger.error(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    print(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    # ê°œë³„ ë°°ì¹˜ ì˜¤ë¥˜ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                    continue
                    
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ë° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ë¬¸ì„œ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            raise

    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            Dict: ë‹µë³€ ê²°ê³¼
        """
        try:
            if not self.is_initialized:
                return {"success": False, "error": "ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
            # 0. ì§ˆë¬¸ ì¸ì½”ë”© ì•ˆì „ ì²˜ë¦¬
            question = self._safe_encode_text(question)
            
            # 1. í•œêµ­ì–´ ì§ˆë¬¸ í™•ì¥
            expanded_query = self._expand_korean_query(question)
            expanded_query = self._safe_encode_text(expanded_query)
            
            # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self._search_relevant_documents(expanded_query)
            
            if not relevant_docs:
                return {"success": False, "error": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._create_context(relevant_docs)
            context = self._safe_encode_text(context)
            
            # 4. AI ë‹µë³€ ìƒì„±
            answer = self._generate_answer(question, context)
            answer = self._safe_encode_text(answer) if answer else ""
            
            if not answer:
                return {"success": False, "error": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"}
            
            # 5. ê²°ê³¼ ë°˜í™˜
            return {
                "success": True,
                "answer": answer,
                "sources": [self._create_source_summary(doc, i+1) for i, doc in enumerate(relevant_docs)],
                "context": context
            }
            
        except Exception as e:
            self.logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    def _safe_encode_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì•ˆì „ ì¸ì½”ë”© ì²˜ë¦¬"""
        if not text:
            return ""
        
        try:
            # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
            if isinstance(text, str):
                # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ë° ì•ˆì „í•œ UTF-8 ì²˜ë¦¬
                import unicodedata
                text = unicodedata.normalize('NFC', text)
                # ì•ˆì „í•œ ë¬¸ìë§Œ ìœ ì§€
                safe_text = ''.join(c for c in text if unicodedata.category(c) not in ['Cc', 'Cf', 'Cs', 'Co', 'Cn'])
                return safe_text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
            
            # ë°”ì´íŠ¸ì¸ ê²½ìš°
            elif isinstance(text, bytes):
                # ì—¬ëŸ¬ ì¸ì½”ë”© ë°©ì‹ì„ ì‹œë„
                for encoding in ['utf-8', 'cp949', 'euc-kr', 'iso-8859-1']:
                    try:
                        decoded = text.decode(encoding)
                        return self._safe_encode_text(decoded)
                    except UnicodeDecodeError:
                        continue
                # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¬´ì‹œ
                return text.decode('utf-8', errors='ignore')
            
            # ê¸°íƒ€ íƒ€ì…ì¸ ê²½ìš°
            else:
                return self._safe_encode_text(str(text))
                
        except Exception as e:
            self.logger.warning(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ì•ˆì „í•œ ë¬¸ìë§Œ ìœ ì§€
            try:
                safe_chars = ''.join(c for c in str(text) if ord(c) < 128 or c.isalnum() or c in ' .,!?-')
                return safe_chars
            except:
                return ""

    def _expand_korean_query(self, query: str) -> str:
        """í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ í™•ì¥"""
        expanded = query
        
        for korean, english in self.korean_to_english.items():
            if korean in query:
                expanded += f" {english}"
        
        return expanded

    def _search_relevant_documents(self, query: str) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_model.encode([query])
            
            # ChromaDBì—ì„œ ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=self.config.top_k
            )
            
            # ê²°ê³¼ ì •ë¦¬
            relevant_docs = []
            for i, doc in enumerate(results['documents'][0]):
                relevant_docs.append({
                    "document": doc,
                    "distance": results['distances'][0][i],
                    "id": results['ids'][0][i]
                })
            
            return relevant_docs
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def _create_context(self, relevant_docs: List[Dict[str, Any]]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []
        
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append(f"[ë¬¸ì„œ {i}]\n{doc['document']}\n")
        
        return "\n".join(context_parts)

    def _create_source_summary(self, doc: Dict[str, Any], index: int) -> str:
        """ê°„ê²°í•œ ì°¸ê³ ë¬¸í—Œ ìš”ì•½ ìƒì„±"""
        text = doc['document']
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ ë˜ëŠ” ì²« 100ì ê°€ì ¸ì˜¤ê¸°
        sentences = text.split('.')
        if len(sentences) > 0 and len(sentences[0]) > 20:
            summary = sentences[0][:100] + "..." if len(sentences[0]) > 100 else sentences[0]
        else:
            summary = text[:100] + "..." if len(text) > 100 else text
        
        # ìš”ì•½ ì •ë¦¬
        summary = summary.strip().replace('\n', ' ').replace('\r', ' ')
        summary = ' '.join(summary.split())  # ì—¬ë¶„ì˜ ê³µë°± ì œê±°
        
        return f"[ì°¸ê³  {index}] {summary}"

    def _generate_answer(self, question: str, context: str) -> str:
        """AI ë‹µë³€ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(question, context)
            
            # Ollama ëª¨ë¸ í˜¸ì¶œ
            response = self.ollama_client.generate(
                model=self.config.model_name,
                prompt=prompt,
                stream=False,
                options={
                    "temperature": self.config.temperature,
                    "top_k": self.config.top_k_generate,
                    "top_p": self.config.top_p,
                    "num_predict": self.config.max_tokens
                }
            )
            
            answer = response.get('response', '').strip()
            
            if not answer:
                return "I'm sorry, I cannot generate an answer based on the available information."
            
            return answer
            
        except Exception as e:
            self.logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"An error occurred while generating the answer: {str(e)}"

    def _create_prompt(self, question: str, context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""You are a medical AI that answers questions based solely on EAU (European Association of Urology) bladder cancer guidelines.

IMPORTANT INSTRUCTIONS:
- Answer ONLY based on the content explicitly stated in the guideline documents provided below
- If information is not found in the documents, respond with "The requested information is not available in the provided guidelines" or "I don't know"
- Do not answer based on external knowledge or speculation
- Do not provide general medical advice not found in the documents
- ALWAYS respond in English, regardless of the language of the question
- Be precise and cite specific sections when possible

EAU Guidelines Context:
{context}

Question: {question}

Answer based solely on the above guideline documents (respond in English):"""
        return prompt

    def get_status(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì •ë³´"""
        try:
            status = {
                "initialized": self.is_initialized,
                "ollama_connected": False,
                "model_available": False,
                "pdf_loaded": self.pdf_loaded,
                "vectordb_ready": False
            }
            
            # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
            if self.ollama_client:
                try:
                    models = self.ollama_client.list()
                    status["ollama_connected"] = True
                    
                    model_names = [model['name'] for model in models['models']]
                    status["model_available"] = self.config.model_name in model_names
                except:
                    pass
            
            # ë²¡í„° DB ìƒíƒœ í™•ì¸
            if self.collection:
                try:
                    count = self.collection.count()
                    status["vectordb_ready"] = count > 0
                    status["document_count"] = count
                except:
                    pass
            
            return status
            
        except Exception as e:
            self.logger.error(f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'embedding_model'):
                del self.embedding_model
            
            if hasattr(self, 'chroma_client'):
                del self.chroma_client
            
            if hasattr(self, 'collection'):
                del self.collection
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")