#!/usr/bin/env python3
"""
BladderCancerAgent - ë°©ê´‘ì•” EAU ê°€ì´ë“œë¼ì¸ AI Agent í•µì‹¬ í´ë˜ìŠ¤
Ollama Qwen ëª¨ë¸ê³¼ RAG ê¸°ëŠ¥ì„ í†µí•©í•œ ë…ë¦½ ì—ì´ì „íŠ¸
"""

import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import unicodedata
import re
import time

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import ollama
import torch
import chromadb
import PyPDF2
from sentence_transformers import SentenceTransformer
from chromadb.utils import embedding_functions
from tqdm import tqdm
import psutil

class BladderCancerAgent:
    """ë°©ê´‘ì•” EAU ê°€ì´ë“œë¼ì¸ AI Agent í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.is_initialized = False
        self.ollama_client = None
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        self.pdf_loaded = False
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config.cache_dir, exist_ok=True)
        
        # ì¸ì½”ë”© ì„¤ì •
        self._setup_encoding()
        
        # í•œêµ­ì–´-ì˜ì–´ ì˜ë£Œìš©ì–´ ë§¤í•‘
        self.korean_to_english = {
            "ë°©ê´‘ì•”": "bladder cancer urothelial carcinoma",
            "ë¹„ê·¼ì¹¨ìœ¤ì„±": "non-muscle invasive NMIBC",
            "ê·¼ì¹¨ìœ¤ì„±": "muscle invasive MIBC",
            "BCG": "BCG bacillus calmette guerin",
            "ë°©ê´‘ê²½": "cystoscopy",
            "ê²½ìš”ë„ë°©ê´‘ì¢…ì–‘ì ˆì œìˆ ": "TURBT transurethral resection",
            "í™”í•™ìš”ë²•": "chemotherapy",
            "ë©´ì—­ìš”ë²•": "immunotherapy",
            "ì¬ë°œ": "recurrence",
            "ë¶€ì‘ìš©": "side effects adverse",
            "ì¹˜ë£Œ": "treatment therapy",
            "ìˆ˜ìˆ ": "surgery operation",
            "ì§„ë‹¨": "diagnosis",
            "ì˜ˆí›„": "prognosis",
            "ìƒì¡´ìœ¨": "survival rate",
            "ë³‘ê¸°": "stage staging",
            "ë“±ê¸‰": "grade grading",
            "ìœ„í—˜ë„": "risk factors",
            "ê°€ì´ë“œë¼ì¸": "guidelines recommendations",
            "í‘œì¤€ì¹˜ë£Œ": "standard treatment",
            "í•­ì•”": "anticancer chemotherapy",
            "ë°©ì‚¬ì„ ": "radiation radiotherapy",
            "ì ˆì œ": "resection removal",
            "ìƒê²€": "biopsy",
            "ì¡°ì§ê²€ì‚¬": "histopathology",
            "ì¢…ì–‘": "tumor neoplasm",
            "ì•…ì„±": "malignant",
            "ì–‘ì„±": "benign",
            "ì „ì´": "metastasis",
            "ì¹¨ìœ¤": "invasion",
            "ìƒí”¼": "epithelium urothelial",
            "ìš”ë¡œ": "urinary tract",
            "ë°°ë‡¨": "urination voiding",
            "í˜ˆë‡¨": "hematuria",
            "ë¹ˆë‡¨": "frequency",
            "ìš”ì ˆë°•": "urgency",
            "ìš”ì‹¤ê¸ˆ": "incontinence"
        }
        
        # ë°©ê´‘ì•” ê´€ë ¨ í‚¤ì›Œë“œ (ê¸°ì¡´ MCP ì„œë²„ì—ì„œ ê°€ì ¸ì˜´)
        self.bladder_keywords = [
            "BCG", "TURBT", "NMIBC", "MIBC", "bladder", "cancer", "urothelial", "carcinoma",
            "cystoscopy", "resection", "chemotherapy", "immunotherapy", "recurrence",
            "survival", "prognosis", "stage", "grade", "risk", "treatment", "therapy",
            "guidelines", "recommendation", "EAU", "urinary", "tract", "hematuria",
            "frequency", "urgency", "incontinence", "biopsy", "histopathology",
            "invasion", "metastasis", "epithelium", "malignant", "benign"
        ]

    def _setup_encoding(self):
        """ì¸ì½”ë”© ì„¤ì •"""
        try:
            import locale
            # ë¡œì¼€ì¼ ì„¤ì •
            if os.name == 'nt':  # Windows
                locale.setlocale(locale.LC_ALL, 'Korean_Korea.utf8')
            else:  # Linux/macOS
                locale.setlocale(locale.LC_ALL, 'ko_KR.utf8')
        except:
            try:
                locale.setlocale(locale.LC_ALL, 'en_US.utf8')
            except:
                pass
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTHONIOENCODING'] = 'utf-8'

    def initialize(self) -> bool:
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘")
            
            # 1. Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if not self._init_ollama():
                return False
            
            # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            if not self._init_embedding_model():
                return False
            
            # 3. ChromaDB ì´ˆê¸°í™”
            if not self._init_chromadb():
                return False
            
            # 4. PDF ë¡œë“œ ë° ë²¡í„°í™”
            if not self._load_pdf_and_vectorize():
                return False
            
            self.is_initialized = True
            self.logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _init_ollama(self) -> bool:
        """Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            
            # Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.ollama_client = ollama.Client(host=self.config.ollama_host)
            
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸
            models = self.ollama_client.list()
            self.logger.info(f"Ollama ì‘ë‹µ: {models}")
            
            # ì•ˆì „í•œ ëª¨ë¸ ëª©ë¡ ì¶”ì¶œ
            if 'models' in models and isinstance(models['models'], list):
                model_names = []
                for model in models['models']:
                    if isinstance(model, dict) and 'name' in model:
                        model_names.append(model['name'])
                    else:
                        self.logger.warning(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ ëª¨ë¸ í˜•ì‹: {model}")
            else:
                self.logger.warning(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ Ollama ì‘ë‹µ í˜•ì‹: {models}")
                model_names = []
            
            if self.config.model_name not in model_names:
                self.logger.info(f"ëª¨ë¸ '{self.config.model_name}'ì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                self.logger.info(f"ì„¤ì¹˜ëœ ëª¨ë¸: {model_names}")
                
                # ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
                if not self._download_model():
                    return False
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            response = self.ollama_client.generate(
                model=self.config.model_name,
                prompt="Hello",
                stream=False
            )
            
            if not response.get('response'):
                self.logger.error("Ollama ëª¨ë¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"Ollama ëª¨ë¸ '{self.config.model_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _download_model(self) -> bool:
        """Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"ğŸ”„ ëª¨ë¸ '{self.config.model_name}' ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 400MB)")
            print("ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # Ollama pull ëª…ë ¹ ì‹¤í–‰
            import subprocess
            import sys
            
            result = subprocess.run(
                ['ollama', 'pull', self.config.model_name], 
                capture_output=True, 
                text=True,
                timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                print(f"âœ… ëª¨ë¸ '{self.config.model_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def _init_embedding_model(self) -> bool:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            model_name = self.config.embedding_model
            self.embedding_model = SentenceTransformer(model_name, device=device)
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_text = "BCG ì¹˜ë£Œì˜ ë¶€ì‘ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            embedding = self.embedding_model.encode([test_text])
            
            if embedding is None or len(embedding) == 0:
                self.logger.error("ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _init_chromadb(self) -> bool:
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            self.logger.info("ChromaDB ì´ˆê¸°í™” ì¤‘...")
            
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.chroma_client = chromadb.PersistentClient(
                path=str(Path(self.config.cache_dir) / "chroma_db")
            )
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
            collection_name = "bladder_cancer_guidelines"
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ (ì¬ìƒì„±)
            try:
                self.chroma_client.delete_collection(collection_name)
            except:
                pass
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                embedding_function=embedding_functions.DefaultEmbeddingFunction()
            )
            
            self.logger.info("ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _load_pdf_and_vectorize(self) -> bool:
        """PDF ë¡œë“œ ë° ë²¡í„°í™”"""
        try:
            self.logger.info("PDF ë¡œë“œ ë° ë²¡í„°í™” ì‹œì‘...")
            
            # PDF íŒŒì¼ ê²½ë¡œ
            pdf_path = Path(self.config.pdf_path)
            if not pdf_path.exists():
                self.logger.error(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                return False
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            documents = self._extract_pdf_text(pdf_path)
            if not documents:
                self.logger.error("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                return False
            
            # ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§
            filtered_docs = self._filter_relevant_documents(documents)
            if not filtered_docs:
                self.logger.error("ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ë²¡í„°í™” ë° ChromaDBì— ì €ì¥
            self._vectorize_and_store(filtered_docs)
            
            self.pdf_loaded = True
            self.logger.info(f"PDF ë²¡í„°í™” ì™„ë£Œ: {len(filtered_docs)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            self.logger.error(f"PDF ë¡œë“œ ë° ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def _extract_pdf_text(self, pdf_path: Path) -> List[str]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            documents = []
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    
                    if text and text.strip():
                        # í…ìŠ¤íŠ¸ ì •ë¦¬
                        cleaned_text = self._clean_text(text)
                        if cleaned_text:
                            documents.append(cleaned_text)
            
            return documents
            
        except Exception as e:
            self.logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s\-\.\,\;\:\(\)\[\]\{\}\'\"\/\%\&\+\=\<\>\!\?\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text

    def _filter_relevant_documents(self, documents: List[str]) -> List[str]:
        """ë°©ê´‘ì•” ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§"""
        filtered = []
        
        for doc in documents:
            # í‚¤ì›Œë“œ ë§¤ì¹­
            doc_lower = doc.lower()
            if any(keyword.lower() in doc_lower for keyword in self.bladder_keywords):
                # ìµœì†Œ ê¸¸ì´ í™•ì¸
                if len(doc.split()) >= 10:
                    filtered.append(doc)
        
        return filtered

    def _vectorize_and_store(self, documents: List[str]):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ChromaDB ì €ì¥"""
        try:
            # ë°°ì¹˜ í¬ê¸° ì„¤ì •
            batch_size = self.config.batch_size
            
            for i in tqdm(range(0, len(documents), batch_size), desc="ë¬¸ì„œ ë²¡í„°í™”"):
                batch = documents[i:i+batch_size]
                
                # ì„ë² ë”© ìƒì„±
                embeddings = self.embedding_model.encode(batch, convert_to_tensor=False)
                
                # ChromaDBì— ì €ì¥
                ids = [f"doc_{i+j}" for j in range(len(batch))]
                self.collection.add(
                    embeddings=embeddings.tolist(),
                    documents=batch,
                    ids=ids
                )
                
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ë° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise

    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            Dict: ë‹µë³€ ê²°ê³¼
        """
        try:
            if not self.is_initialized:
                return {"success": False, "error": "ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
            # 1. í•œêµ­ì–´ ì§ˆë¬¸ í™•ì¥
            expanded_query = self._expand_korean_query(question)
            
            # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self._search_relevant_documents(expanded_query)
            
            if not relevant_docs:
                return {"success": False, "error": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._create_context(relevant_docs)
            
            # 4. AI ë‹µë³€ ìƒì„±
            answer = self._generate_answer(question, context)
            
            if not answer:
                return {"success": False, "error": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"}
            
            # 5. ê²°ê³¼ ë°˜í™˜
            return {
                "success": True,
                "answer": answer,
                "sources": [doc["document"] for doc in relevant_docs],
                "context": context
            }
            
        except Exception as e:
            self.logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    def _expand_korean_query(self, query: str) -> str:
        """í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ í™•ì¥"""
        expanded = query
        
        for korean, english in self.korean_to_english.items():
            if korean in query:
                expanded += f" {english}"
        
        return expanded

    def _search_relevant_documents(self, query: str) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_model.encode([query])
            
            # ChromaDBì—ì„œ ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=self.config.top_k
            )
            
            # ê²°ê³¼ ì •ë¦¬
            relevant_docs = []
            for i, doc in enumerate(results['documents'][0]):
                relevant_docs.append({
                    "document": doc,
                    "distance": results['distances'][0][i],
                    "id": results['ids'][0][i]
                })
            
            return relevant_docs
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def _create_context(self, relevant_docs: List[Dict[str, Any]]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []
        
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append(f"[ë¬¸ì„œ {i}]\n{doc['document']}\n")
        
        return "\n".join(context_parts)

    def _generate_answer(self, question: str, context: str) -> str:
        """AI ë‹µë³€ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(question, context)
            
            # Ollama ëª¨ë¸ í˜¸ì¶œ
            response = self.ollama_client.generate(
                model=self.config.model_name,
                prompt=prompt,
                stream=False,
                options={
                    "temperature": self.config.temperature,
                    "top_k": self.config.top_k_generate,
                    "top_p": self.config.top_p,
                    "num_predict": self.config.max_tokens
                }
            )
            
            answer = response.get('response', '').strip()
            
            if not answer:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return answer
            
        except Exception as e:
            self.logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _create_prompt(self, question: str, context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¹ì‹ ì€ ë°©ê´‘ì•” ì¹˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. EAU(European Association of Urology) ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ë£Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì•„ë˜ ì œê³µëœ ì˜ë£Œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ã€ì°¸ê³  ë¬¸ì„œã€‘
{context}

ã€ì§ˆë¬¸ã€‘
{question}

ã€ë‹µë³€ ì§€ì¹¨ã€‘
1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ì˜í•™ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ "ê°€ì´ë“œë¼ì¸ì— ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. êµ¬ì²´ì ì¸ ì¹˜ë£Œë²•ì´ë‚˜ ì•½ë¬¼ ì •ë³´ëŠ” ì „ë¬¸ì˜ì™€ ìƒë‹´í•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ã€ë‹µë³€ã€‘
"""
        return prompt

    def get_status(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì •ë³´"""
        try:
            status = {
                "initialized": self.is_initialized,
                "ollama_connected": False,
                "model_available": False,
                "pdf_loaded": self.pdf_loaded,
                "vectordb_ready": False
            }
            
            # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
            if self.ollama_client:
                try:
                    models = self.ollama_client.list()
                    status["ollama_connected"] = True
                    
                    model_names = [model['name'] for model in models['models']]
                    status["model_available"] = self.config.model_name in model_names
                except:
                    pass
            
            # ë²¡í„° DB ìƒíƒœ í™•ì¸
            if self.collection:
                try:
                    count = self.collection.count()
                    status["vectordb_ready"] = count > 0
                    status["document_count"] = count
                except:
                    pass
            
            return status
            
        except Exception as e:
            self.logger.error(f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'embedding_model'):
                del self.embedding_model
            
            if hasattr(self, 'chroma_client'):
                del self.chroma_client
            
            if hasattr(self, 'collection'):
                del self.collection
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")